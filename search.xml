<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[怎么在iPhone设置汕大邮箱]]></title>
    <url>%2F2018%2F03%2F25%2F%E5%9C%A8iPhone%E4%B8%8A%E8%AE%BE%E7%BD%AE%E6%B1%95%E5%A4%B4%E5%A4%A7%E5%AD%A6%E9%82%AE%E7%AE%B1%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1.在iPhone上下载QQ邮箱应用。2.进入QQ邮箱：选择“添加账户” -&gt; 选择“其他邮箱” -&gt; 输入你在汕大的“邮箱名+邮箱密码”;3.此处需要填写服务器等信息，表明你的邮箱已经运行在这个服务器上，而且数据时加密的： 新邮件服务器的SMTP服务器设置 SMTP服务器地址： smtp.partner.outlook.cn 端口：587 加密方法：TLS4.配置完成，祝你珍惜汕大时光，好好生活，好好学习。 附：1.安卓手机配置也是一样的流程。 2.配置邮箱也可以在iPhone自带邮箱应用上进行，或者在“设置”的“账户与密码”里面进行设置。 3.新邮件系统不支持POP和IMAP，只支持Exchange协议。]]></content>
  </entry>
  <entry>
    <title><![CDATA[sklearn.svm.SVC参数设置详解]]></title>
    <url>%2F2018%2F03%2F22%2Fsklearn-svm-SVC%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[sklearn.svm.SVC 参数说明 这个函数也是基于libsvm实现的。参数列表如下：sklearn.svm.SVC(C=1.0, kernel=’rbf’, degree=3, gamma=’auto’, coef0=0.0, shrinking=True, probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None,random_state=None) 参数详解如下： l C：C-SVC的惩罚参数C?默认值是1.0 C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样对训练集测试时准确率很高，但泛化能力弱。C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。 l kernel ：核函数，默认是rbf，可以是‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ 0 – 线性：u’v 1 – 多项式：(gammau’v + coef0)^degree 2 – RBF函数：exp(-gamma|u-v|^2) 3 –sigmoid：tanh(gammau’v + coef0) l degree ：多项式poly函数的维度，默认是3，选择其他核函数时会被忽略。 l gamma ： ‘rbf’,‘poly’ 和‘sigmoid’的核函数参数。默认是’auto’，则会选择1/n_features l coef0 ：核函数的常数项。对于‘poly’和 ‘sigmoid’有用。 l probability ：是否采用概率估计？.默认为False l shrinking ：是否采用shrinking heuristic方法，默认为true l tol ：停止训练的误差值大小，默认为1e-3 l cache_size ：核函数cache缓存大小，默认为200 l class_weight ：类别的权重，字典形式传递。设置第几类的参数C为weight*C(C-SVC中的C) l verbose ：允许冗余输出？ l max_iter ：最大迭代次数。-1为无限制。 l decision_function_shape ：‘ovo’, ‘ovr’ or None, default=None3 l random_state ：数据洗牌时的种子值，int值 主要调节的参数有：C、kernel、degree、gamma、coef0。]]></content>
  </entry>
  <entry>
    <title><![CDATA[基础篇二进制位操作]]></title>
    <url>%2F2018%2F03%2F17%2F%E5%9F%BA%E7%A1%80%E7%AF%87%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%BD%8D%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一． 位操作基础基本的位操作符有与、或、异或、取反、左移、右移这6种，它们的运算规则如下所示： &amp; 与 两个位都为1时，结果才为1 | 或 两个位都为0时，结果才为0 ^ 异或 两个位相同为0，相异为1 ~ 取反 0变1，1变0 &lt;&lt; 左移 各二进位全部左移若干位，高位丢弃，低位补0 右移 各二进位全部右移若干位，对无符号数，高位补0，有符号数，各编译器处理方法不一样，有的补符号位（算术右移），有的补0（逻辑右移） 注意以下几点： 1． 在这6种操作符，只有~取反是单目操作符，其它5种都是双目操作符。 2． 位操作只能用于整形数据，对float和double类型进行位操作会被编译器报错。 3． 对于移位操作，比如15 = 0000 1111(二进制)，右移二位，最高位由符号位填充将得到0000 0011即3。 -15 = 1111 0001(二进制)，右移二位，最高位由符号位填充将得到1111 1100即-4。 4． 位操作符的运算优先级比较低，因为尽量使用括号来确保运算顺序。比如要得到像1，3，5，9这些2^i+1的数字。写成int a = 1 &lt;&lt; i + 1;是不对的，程序会先执行i + 1，再执行左移操作。应该写成int a = (1 &lt;&lt; i) + 1; 5． 另外位操作还有一些复合操作符，如&amp;=、|=、 ^=、&lt;&lt;=、&gt;&gt;=。 二． 常用位操作小技巧下面对位操作的一些常见应用作个总结，有判断奇偶、交换两数、变换符号及求绝对值。这些小技巧应用易记，应当熟练掌握。 1．判断奇偶只要根据最未位是0还是1来决定，为0就是偶数，为1就是奇数。因此可以用if ((a &amp; 1) == 0)代替if (a % 2 == 0)来判断a是不是偶数。 下面程序将输出0到100之间的所有奇数。for (i = 0; i &lt; 100; ++i) if (i &amp; 1) printf(“%d “, i);putchar(‘\n’); 2．交换两数一般的写法是：void Swap(int &amp;a, int &amp;b){ if (a != b) { int c = a; a = b; b = c; }}可以用位操作来实现交换两数而不用第三方变量：void Swap(int &amp;a, int &amp;b){ if (a != b) { a ^= b; b ^= a; a ^= b; }}可以这样理解： 第一步 a^=b 即a=(a^b); 第二步 b^=a 即b=b^(a^b)，由于^运算满足交换律，b^(a^b)=b^b^a。由于一个数和自己异或的结果为0并且任何数与0异或都会不变的，所以此时b被赋上了a的值。 第三步 a^=b 就是a=a^b，由于前面二步可知a=(a^b)，b=a，所以a=a^b即a=(a^b)^a。故a会被赋上b的值。 再来个实例说明下以加深印象。int a = 13, b = 6; a的二进制为 13=8+4+1=1101(二进制) b的二进制为 6=4+2=110(二进制) 第一步 a^=b a = 1101 ^ 110 = 1011; 第二步 b^=a b = 110 ^ 1011 = 1101;即b=13 第三步 a^=b a = 1011 ^ 1101 = 110;即a=6 3．变换符号变换符号就是正数变成负数，负数变成正数。 如对于-11和11，可以通过下面的变换方法将-11变成11 1111 0101(二进制) –取反-&gt; 0000 1010(二进制) –加1-&gt; 0000 1011(二进制) 同样可以这样的将11变成-11 0000 1011(二进制) –取反-&gt; 0000 0100(二进制) –加1-&gt; 1111 0101(二进制) 因此变换符号只需要取反后加1即可。完整代码如下： #include int SignReversal(int a){ return ~a + 1;}int main(){ printf(“对整数变换符号 — by More —\n\n”); int a = 7, b = -12345; printf(“%d %d\n”, SignReversal(a), SignReversal(b)); return 0;}4．求绝对值位操作也可以用来求绝对值，对于负数可以通过对其取反后加1来得到正数。对-6可以这样： 1111 1010(二进制) –取反-&gt;0000 0101(二进制) -加1-&gt; 0000 0110(二进制) 来得到6。 因此先移位来取符号位，int i = a &gt;&gt; 31;要注意如果a为正数，i等于0，为负数，i等于-1。然后对i进行判断——如果i等于0，直接返回。否之，返回~a+1。完整代码如下： int my_abs(int a){ int i = a &gt;&gt; 31; return i == 0 ? a : (~a + 1);}现在再分析下。对于任何数，与0异或都会保持不变，与-1即0xFFFFFFFF异或就相当于取反。因此，a与i异或后再减i（因为i为0或-1，所以减i即是要么加0要么加1）也可以得到绝对值。所以可以对上面代码优化下： int myabs(int a){ int i = a &gt;&gt; 31; return ((a ^ i) - i);}注意这种方法没用任何判断表达式，而且有些笔面试题就要求这样做，因此建议读者记住该方法（^^讲解过后应该是比较好记了）。 注1．int类型一般占4字节，32位。因此15准确表达为 15=00000000 00000000 00000000 00001111(二进制) -15准确表达为 -15=11111111 11111111 11111111 11110001(二进制) 为了简便起见，文章中使用15=00001111(二进制)，-15=11110001(二进制)。 应用． 二进制中1的个数 统计二进制中1的个数可以直接移位再判断，当然像《编程之美》书中用循环移位计数或先打一个表再计算都可以。本文详细讲解一种高效的方法。以34520为例，可以通过下面四步来计算其二进制中1的个数二进制中1的个数。 第一步：每2位为一组，组内高低位相加 10 00 01 10 11 01 10 00 –&gt;01 00 01 01 10 01 01 00 第二步：每4位为一组，组内高低位相加 0100 0101 1001 0100 –&gt;0001 0010 0011 0001 第三步：每8位为一组，组内高低位相加 00010010 00110001 –&gt;00000011 00000100 第四步：每16位为一组，组内高低位相加 00000011 00000100 –&gt;00000000 00000111 这样最后得到的00000000 00000111即7即34520二进制中1的个数。类似上文中对二进制逆序的做法不难实现第一步的代码： x = ((x &amp; 0xAAAA) &gt;&gt; 1) + (x &amp; 0x5555); 好的，有了第一步，后面几步就请读者完成下吧，先动动笔再看下面的完整代码： #include template void PrintfBinary(T a){ int i; for (i = sizeof(a) * 8 - 1; i &gt;= 0; –i) { if ((a &gt;&gt; i) &amp; 1) putchar(‘1’); else putchar(‘0’); if (i == 8) putchar(‘ ‘); } putchar(‘\n’);}int main(){ printf(“二进制中1的个数 — by MoreWindows( http://blog.csdn.net/MoreWindows ) —\n\n”); unsigned short a = 34520; printf(&quot;原数 %6d的二进制为: &quot;, a); PrintfBinary(a); a = ((a &amp; 0xAAAA) &gt;&gt; 1) + (a &amp; 0x5555); a = ((a &amp; 0xCCCC) &gt;&gt; 2) + (a &amp; 0x3333); a = ((a &amp; 0xF0F0) &gt;&gt; 4) + (a &amp; 0x0F0F); a = ((a &amp; 0xFF00) &gt;&gt; 8) + (a &amp; 0x00FF); printf(&quot;计算结果%6d的二进制为: &quot;, a); PrintfBinary(a); return 0; }运行结果如下： 可以发现巧妙运用分组处理确实是解决很多二进制问题的灵丹妙药。 转载自：http://blog.csdn.net/morewindows/article/details/7354571感谢原作者]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>位操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之支持向量机]]></title>
    <url>%2F2018%2F01%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[机器学习实战之SVM 支持向量机这部分确实很多，想要真正的去理解它，不仅仅知道理论，还要进行相关的代码编写和测试，二者想和结合，才能更好的帮助我们理解SVM这一非常优秀的分类算法 支持向量机是一种二类分类算法，假设一个平面可以将所有的样本分为两类，位于正侧的样本为一类，值为+1，而位于负一侧的样本为另外一类，值为-1。 我们说分类，不仅仅是将不同的类别样本分隔开，还要以比较大的置信度来分隔这些样本，这样才能使绝大部分样本被分开。比如，我们想通过一个平面将两个类别的样本分开，如果这些样本是线性可分（或者近视线性可分），那么这样的平面有很多，但是如果我们加上要以最大的置信度来将这些样本分开，那么这样的平面只有一条。那么怎么才能找到这样的平面呢？这里不得不提到几个概念 1 几何间隔 几何间隔的概念，简单理解就是样本点到分隔平面的距离 2 间隔最大化 想要间隔最大化，我们必须找到距离分隔平面最近的点，并且使得距离平面最近的点尽可能的距离平面最远，这样，每一个样本就都能够以比较大的置信度被分隔开 算法的分类预测能力也就越好 显然，SVM算法的关键所在，就是找到使得间隔最大化的分隔超平面（如果特征是高维度的情况，我们称这样的平面为超平面） 这里关于SVM学习，推荐两本书：统计学习方法（李航）和机器学习实战，二者结合，可以帮助我们理解svm算法 2 支持向量机 支持向量机的推导，可以看李航的统计与学习方法书，写的浅显易懂。 这两位博主都重点讲解了SVM的推导过程。这里我就本着站在巨人的肩膀上的思想，不再赘述，我的侧重点在于实际的代码编写上，比较理论总归要回到实践上，这也是每个算法的归宿所在。 好了，下面我就简要写出简要介绍一下，线性支持向量机，近似线性支持向量机以及非线性支持向量机（核函数） 1 线性支持向量机 求解线性支持向量机的过程是凸二次规划问题，所谓凸二次规划问题，就是目标函数是凸的二次可微函数，约束函数为仿射函数（满足f(x)=ax+b,a,x均为n为向量）。而我们说求解凸二次规划问题可以利用对偶算法–即引入拉格朗日算子，利用拉格朗日对偶性将原始问题的最优解问题转化为拉格朗日对偶问题，这样就将求w，b的原始问题的极小问题转化为求alpha（alpha&gt;=0）的对偶问题的极大问题，即求出alpha，在通过KKT条件求出对应的参数w*，b，从而找到这样的间隔最大化超平面，进而利用该平面完成样本分类 2 近似线性支持向量机 当数据集并不是严格线性可分时，即满足绝不部分样本点是线性可分，存在极少部分异常点；这里也就是说存在部分样本不能满足约束条件，此时我们可以引入松弛因子，这样这些样本点到超平面的函数距离加上松弛因子，就能保证被超平面分隔开来；当然，添加了松弛因子sigma，我们也会添加对应的代价项，使得alpha满足0=&lt;alpha&lt;=C 3 非线性支持向量机 显然，当数据集不是线性可分的，即我们不能通过前面的线性模型来对数据集进行分类。此时，我们必须想办法将这些样本特征符合线性模型，才能通过线性模型对这些样本进行分类。这就要用到核函数，核函数的功能就是将低维的特征空间映射到高维的特征空间，而在高维的特征空间中，这些样本进过转化后，变成了线性可分的情况，这样，在高维空间中，我们就能够利用线性模型来解决数据集分类问题 好了，我们就只讲这么写大致的概念，如果想要透彻理解SVM建议还是要看看上面的书和博客文章，篇幅有限，我这里的中心在于凸二次规划的优化算法–SMO(序列最小最优化算法) 3 SMO算法 SMO是一种用于训练SVM的强大算法，它将大的优化问题分解为多个小的优化问题来进行求解。而这些小优化问题往往很容易求解，并且对它们进行顺序求解和对整体求解结果是一致的。在结果一致的情况下，显然SMO算法的求解时间要短很多，这样当数据集容量很大时，SMO就是一致十分高效的算法 SMO算法的目标是找到一系列alpha和b，而求出这些alpha，我们就能求出权重w，这样就能得到分隔超平面，从而完成分类任务 SMO算法的工作原理是：每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个而减少另外一个。这里的”合适”,意味着在选择alpha对时必须满足一定的条件，条件之一是这两个alpha不满足最优化问题的kkt条件，另外一个条件是这两个alpha还没有进行区间化处理 对于SMO算法编写，我们采用由简单到复杂的方法，层层递进，完成最终的SMO算法实现，最后通过实际的用例对SVM模型进行训练，并验证准确性 1 简化版SMO算法 简化版SMO算法，省略了确定要优化的最佳alpha对的步骤，而是首先在数据集上进行遍历每一个alpha，再在剩余的数据集中找到另外一个alpha，构成要优化的alpha对，同时对其进行优化，这里的同时是要确保公式：Σαi*label(i)=0。所以改变一个alpha显然会导致等式失效，所以这里需要同时改变两个alpha。接下来看实际的代码： 简易版SMO算法的辅助函数： 复制代码 #SMO算法相关辅助中的辅助函数 #1 解析文本数据函数，提取每个样本的特征组成向量，添加到数据矩阵 #添加样本标签到标签向量def loadDataSet(filename): dataMat=[];labelMat=[] fr=open(filename) for line in fr.readlines(): lineArr=line.strip().split(‘\t’) dataMat.append([float(lineArr[0]),float(lineArr[1])]) labelMat.append((float()lineArr[2])) return dataMat,labelMat #2 在样本集中采取随机选择的方法选取第二个不等于第一个alphai的 #优化向量alphajdef selectJrand(i,m): j=i while(j==i): j=int(random.uniform(0,m)) return j #3 约束范围L&lt;=alphaj&lt;=H内的更新后的alphaj值def clipAlpha(aj,H,L): if aj&gt;H: aj=H if L&gt;aj： aj=L return aj复制代码上面是简易版SMO算法需要用到的一些功能，我们将其包装成函数，需要时调用即可，接下来看算法的伪代码： 复制代码 #SMO算法的伪代码 #创建一个alpha向量并将其初始化为0向量 #当迭代次数小于最大迭代次数时(w外循环) #对数据集中每个数据向量(内循环): #如果该数据向量可以被优化： #随机选择另外一个数据向量 #同时优化这两个向量 #如果两个向量都不能被优化，退出内循环 #如果所有向量都没有被优化，增加迭代次数，继续下一次循环复制代码实际代码如下： 复制代码 #@dataMat ：数据列表 #@classLabels：标签列表 #@C ：权衡因子（增加松弛因子而在目标优化函数中引入了惩罚项） #@toler ：容错率 #@maxIter ：最大迭代次数def smoSimple(dataMat,classLabels,C,toler,maxIter): #将列表形式转为矩阵或向量形式 dataMatrix=mat(dataMatIn);labelMat=mat(classLabels).transpose() #初始化b=0，获取矩阵行列 b=0;m,n=shape(dataMatrix) #新建一个m行1列的向量 alphas=mat(zeros((m,1))) #迭代次数为0 iter=0 while(iter&lt;maxIter): #改变的alpha对数 alphaPairsChanged=0 #遍历样本集中样本 for i in range(m): #计算支持向量机算法的预测值 fXi=float(multiply(alphas,labelMat).T*\ (dataMatrix*dataMatrix[i,:].T))+b #计算预测值与实际值的误差 Ei=fXi-float(labelMat[i]) #如果不满足KKT条件，即labelMat[i]*fXi&lt;1(labelMat[i]*fXi-1&lt;-toler) #and alpha&lt;C 或者labelMat[i]*fXi&gt;1(labelMat[i]*fXi-1&gt;toler)and alpha&gt;0 if((labelMat[i]*Ei&lt;-toler)and(alpha&lt;C))or\ ((labelMat[i]*Ei&gt;toler)and(alpha[i]&gt;0))): #随机选择第二个变量alphaj j=selectJrand(i,m) #计算第二个变量对应数据的预测值 fXj=float(multiply(alphas,labelMat).T*\ (dataMatrix*dataMatrix[j,:]).T)+b #计算与测试与实际值的差值 Ej=fXj-float(label[j]) #记录alphai和alphaj的原始值，便于后续的比较 alphaIold=alphas[i].copy() alphaJold=alphas[j].copy() #如何两个alpha对应样本的标签不相同 if(labelMat[i]!=labelMat[j]): #求出相应的上下边界 L=max(0,alphas[j]-alphas[i]) H=min(C,C+alphas[j]-alphas[i]) else: L=max(0,alphas[j]+alphas[i]-C) H=min(C,alphas[j]+alphas[i]) if L==H:print(&quot;L==H);continue #根据公式计算未经剪辑的alphaj #------------------------------------------ eta=2.0*dataMatrix[i,:]*dataMatrix[j,:].T-\ dataMatrix[i,:]*dataMatrix[i,:].T-\ dataMatrix[j,:]*dataMatrix[j,:].T #如果eta&gt;=0,跳出本次循环 if eta&gt;=0:print(&quot;eta&gt;=0&quot;):continue alphas[j]-=labelMat[j]*(Ei-Ej)/eta alphas[j]=clipAlpha(alphas[j],H,L) #------------------------------------------ #如果改变后的alphaj值变化不大，跳出本次循环 if(abs(alphas[j]-alphaJold)&lt;0.00001):print(&quot;j not moving\ enough&quot;);continue #否则，计算相应的alphai值 alphas[i]+=labelMat[j]*labelMat[i]*(alphaJold-alphas[j]) #再分别计算两个alpha情况下对于的b值 b1=b-Ei-labelMat[i]*(alphas[i]-alphaIold)*\ dataMatrix[i,:]*dataMat[i,:].T-\ labelMat[j]*(alphas[j]-alphaJold)*\ dataMatrix[i,:]*dataMatrix[j,:].T b2=b-Ej-labelMat[i]*(alphas[i]-alphaIold)*\ dataMatrix[i,:]*dataMatrix[j,:].T-\ labelMat[j]*(alphas[j]-alphaJold)*\ dataMatrix[j,:]*dataMatrix[j,:].T #如果0&lt;alphai&lt;C,那么b=b1 if(0&lt;alphas[i]) and (C&gt;alphas[i]):b=b1 #否则如果0&lt;alphai&lt;C,那么b=b1 elif (0&lt;alphas[j]) and (C&gt;alphas[j]):b=b2 #否则，alphai，alphaj=0或C else:b=(b1+b2)/2.0 #如果走到此步，表面改变了一对alpha值 alphaPairsChanged+=1 print(&quot;iter: &amp;d i:%d,paird changed %d&quot;,%(iter,i,alphaPairsChanged)) #最后判断是否有改变的alpha对，没有就进行下一次迭代 if(alphaPairsChanged==0):iter+=1 #否则，迭代次数置0，继续循环 else:iter=0 print(&quot;iteration number: %d&quot; %iter) #返回最后的b值和alpha向量 return b,alphas 复制代码 上面的代码量看起来很多，但事实上只要理解了SVM算法的理论知识，就很容易理解，其只不过是将理论转化为机器可以运行的语言而已。 上面代码在一台性能一般的笔记本上对100个样本的数据集上运行，收敛时间14.5秒，取得了令人满意的分类效果 当然，上面的代码通过对整个数据集进行两次遍历的方法来寻找alpha对的方法，显然存在一定的不足，如果数据集规模较小的情况下，或许还可以满足要求。但是对于大规模的数据集而言，上面的代码显然收敛速度非常慢，所以，接下来我们在此基础上对选取合适的alpha对方法进行改进，采用启发式的方法来选取合适的alpha对，从而提升运算效率。 2 启发式选取alpha变量的SMO算法 启发式的SMO算法一个外循环来选择第一个alpha值，并且其选择过程会在下面两种方法之间进行交替： （1）在所有数据集上进行单遍扫描 （2）另一种方法是在间隔边界上样本点进行单遍扫描，所谓间隔边界上的点即为支持向量点。 显然，对于整个数据集遍历比较容易，而对于那些处于间隔边界上的点，我们还需要事先将这些点对应的alpha值找出来，存放在一个列表中，然后对列表进行遍历；此外，在选择第一个alpha值后，算法会通过一个内循环来选择第二个值，在优化的过程中依据alpha的更新公式αnew,unc=aold+label*(Ei-Ej)/η，(η=dataMat[i,:]*dataMat[i,:].T+dataMat[j,:]*dataMat[j,:].T-2*dataMat[i,:]*dataMat[j,:].T),可知alpha值的变化程度更Ei-Ej的差值成正比，所以，为了使alpha有足够大的变化，选择使Ei-Ej最大的alpha值作为另外一个alpha。所以，我们还可以建立一个全局的缓存用于保存误差值，便于我们选择合适的alpha值 下面是创建的一个数据结构类，便于我们存取算法中需要用到的重要数据： 复制代码 #启发式SMO算法的支持函数 #新建一个类的收据结构，保存当前重要的值class optStruct: def init(self,dataMatIn,classLabels,C,toler): self.X=dataMatIn self.labelMat=classLabels self.C=C self.tol=toler self.m=shape(dataMatIn)[0] self.alphas=mat(zeros((self.m,1))) self.b=0 self.eCache=mat(zeros((self.m,2))) #格式化计算误差的函数，方便多次调用def calcEk(oS,k): fXk=float(multiply(oS.alphas,oS.labelMat).T\ (oS.XoS.X[k,:].T))+oS.b Ek=fXk-float(oS.labelMat[k]) return Ek #修改选择第二个变量alphaj的方法def selectJ(i,oS,Ei): maxK=-1;maxDeltaE=-;Ej=0 #将误差矩阵每一行第一列置1，以此确定出误差不为0 #的样本 oS.eCache[i]=[1,Ei] #获取缓存中Ei不为0的样本对应的alpha列表 validEcacheList=nonzero(oS.Cache[:,0].A)[0] #在误差不为0的列表中找出使abs(Ei-Ej)最大的alphaj if(len(validEcacheList)&gt;0): for k in validEcacheList: if k ==i:continue Ek=calcEk(oS,k) deltaE=abs(Ei-Ek) if(deltaE&gt;maxDeltaE): maxK=k;maxDeltaE=deltaE;Ej=Ek return maxK,Ej else: #否则，就从样本集中随机选取alphaj j=selectJrand(i,oS.m) Ej=calcEk(oS,j) return j,Ej #更新误差矩阵def updateEk(oS,k): Ek=calcEk(oS,k) oS.eCache[k]=[1,Ek]复制代码 好了，有了这些辅助性的函数，我们就可以很容易的实现启发式的SMO算法的具体代码： 复制代码 #SMO外循环代码def smoP(dataMatIn,classLabels,C,toler,maxIter,kTup=(‘lin’,0))： #保存关键数据 oS=optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler) iter=0 enrireSet=True;alphaPairsChanged=0 #选取第一个变量alpha的三种情况，从间隔边界上选取或者整个数据集 while(iter&lt;maxIter)and((alphaPairsChanged&gt;0)or(entireSet)): alphaPairsChanged=0 #没有alpha更新对 if entireSet: for i in range(oS.m): alphaPairsChanged+=innerL(i,oS) print(&quot;fullSet,iter: %d i:%d,pairs changed %d&quot;,%\ (iter,i,alphaPairsChanged)) else: #统计alphas向量中满足0&lt;alpha&lt;C的alpha列表 nonBoundIs=nonzero((oS.alphas.A)&gt;0)*(oS.alphas.A&lt;C))[0] for i in nonBoundIs: alphaPairsChanged+=innerL(i,oS) print(&quot;non-bound,iter: %d i:%d,pairs changed %d&quot;,%\ (iter,i,alphaPairsChanged)) iter+=1 if entireSet:entireSet=False #如果本次循环没有改变的alpha对，将entireSet置为true， #下个循环仍遍历数据集 elif (alphaPairsChanged==0):entireSet=True print(&quot;iteration number: %d&quot;,%iter) return oS.b,oS.alphas #内循环寻找alphajdef innerL(i,oS): #计算误差 Ei=calcEk(oS,i) #违背kkt条件 if(((oS.labelMat[i]*Ei&lt;-oS.tol)and(oS.alphas[i]&lt;oS.C))or\ ((oS.labelMat[i]*Ei&gt;oS.tol)and(oS.alphas[i]&gt;0))): j,Ej=selectJ(i,oS,Ei) alphaIold=alphas[i].copy();alphaJold=alphas[j].copy() #计算上下界 if(oS.labelMat[i]!=oS.labelMat[j]): L=max(0,oS.alphas[j]-oS.alphas[i]) H=min(oS.C,oS.C+oS.alphas[j]-oS.alphas[i]) else: L=max(0,oS.alphas[j]+oS.alphas[i]-oS.C) H=min(oS.C,oS.alphas[j]+oS.alphas[i]) if L==H:print(&quot;L==H&quot;);return 0 #计算两个alpha值 eta=2.0*oS.X[i,:]*oS.X[j,:].T-oS.X[i,:]*oS.X[i,:].T-\ oS.X[j,:]*oS.X[j,:].T if eta&gt;=0:print(&quot;eta&gt;=0&quot;);return 0 oS.alphas[j]-=oS.labelMat[j]*(Ei-Ej)/eta oS.alphas[j]=clipAlpha(oS.alphas[j],H,L) updateEk(oS,j) if(abs(oS.alphas[j]-alphaJold)&lt;0.00001): print(&quot;j not moving enough&quot;);return 0 oS.alphas[i]+=oS.labelMat[j]*oS.labelMat[i]*\ (alphaJold-oS.alphas[j]) updateEk(oS,i) #在这两个alpha值情况下，计算对应的b值 #注，非线性可分情况，将所有内积项替换为核函数K[i,j] b1=oS.b-Ei-oS.labelMat[i]*(oS.alphas[i]-alphaIold)*\ oS.X[i,:]*oS.X[i,:].T-\ oS.labelMat[j]*(oS.alphas[j]-alphaJold)*\ oS.X[i,:]*oS.X[j,:].T b2=oS.b-Ej-oS.labelMat[i]*(oS.alphas[i]-alphaIold)*\ oS.X[i,:]*oS.X[j,:].T-\ oS.labelMat[j]*(oS.alphas[j]-alphaJold)*\ oS.X[j,:]*oS.X[j,:].T if(0&lt;oS.alphas[i])and (oS.C&gt;oS.alphas[i]):oS.b=b1 elif(0&lt;oS.alphas[j])and (oS.C&gt;oS.alphas[j]):oS.b=b2 else:oS.b=(b1+b2)/2.0 #如果有alpha对更新 return 1 #否则返回0 else return 0 复制代码 显然，上面的SMO完整代码是分为内外两个循环函数来编写的，采取这样的结构可以更方便我们去理解选取两个alpha的过程；既然，我们已经计算出了alpha值和b值，那么显然我们可以利用公式w=Σαilabel[i]dataMat[i,:]计算出相应的权值参数，然后就可以得到间隔超平面的公式wx+b来完成样本的分类了，由于SVM算法是一种二类分类算法，正值为1，负值为-1，即分类的决策函数为跳跃函数sign（wx+b*） 然后，我们可以编写一小段测试代码，来利用SMO算法得到的alpha值和b值，计算分类决策函数，从而实现具体的预测分类了 复制代码 #求出了alpha值和对应的b值，就可以求出对应的w值，以及分类函数值def predict(alphas,dataArr,classLabels): X=mat(dataArr);labelMat=mat(classLabels) m,n=shape(X) w=zeros((n,1)) for i in range(m): w+=multiply(alphas[i]labelMat[i],X[i,:].T) result=dataArr[0]mat(ws)+b return sign(result) 复制代码 看一下分类效果： 3 核函数 核函数的目的主要是为了解决非线性分类问题，通过核技巧将低维的非线性特征转化为高维的线性特征，从而可以通过线性模型来解决非线性的分类问题。 如下图，当数据集不是线性可分时，即数据集分布是下面的圆形该怎么办呢？ 显然，此时数据集线性不可分，我们无法用一个超平面来将两种样本分隔开；那么我们就希望将这些数据进行转化，转化之后的数据就能够通过一个线性超平面将不同类别的样本分开，这就需要核函数，核函数的目的主要是为了解决非线性分类问题，通过核技巧将低维的非线性特征转化为高维的线性特征，从而可以通过线性模型来解决非线性的分类问题。 而径向基核函数，是SVM中常用的一个核函数。径向基核函数是一个采用向量作为自变量的函数，能够基于向量距离运算输出一个标量。径向基核函数的高斯版本公式为： k(x，y)=exp(-||x-y||2/2σ2),其中，σ为到达率，决定了函数值跌落至0的速度 下面通过代码编写高斯核函数： 复制代码 #径向基核函数是svm常用的核函数 #核转换函数def kernelTrans(X,A,kTup): m,n=shape(X) K=mat(zeros((m,1))) #如果核函数类型为&apos;lin&apos; if kTup[0]==&apos;lin&apos;:K=X*A.T #如果核函数类型为&apos;rbf&apos;:径向基核函数 #将每个样本向量利用核函数转为高维空间 elif kTup[0]==&apos;rbf&apos; for j in range(m): deltaRow=X[j,:]-A K[j]=deltaRow*deltaRow.T K=exp(K/(-1*kTup[1]**2)) else：raise NameError(&apos;Houston we Have a Problem -- \ That Kernel is not recognised&apos;) return K #对核函数处理的样本特征，存入到optStruct中class optStruct： def init(self,dataMatIn,classLabels,C,toler,kTup): self.X=dataMatIn self.labelMat=classLabels self.C=C self.tol=toler self.m=shape(dataMatIn)[0] self.alphas=mat(zeros((self.m,1))) self.b=0 self.eCache=mat(zeros((self.m,2))) self.K=mat(zeros((self.m,self.m))) for i in range(self.m): self.K[:,i]=kernelTrans(self.X,self.X[i,:],kTup)复制代码需要说明的是，这里引入了一个变量kTup,kTup是一个包含核信息的元组，它提供了选取的核函数的类型，比如线性’lin’或者径向基核函数’rbf’;以及用户提供的到达率σ 有了高斯核函数之后，我们只要将上面的SMO算法中所有的内积项替换为核函数即可，比如讲dataMat[i,:]*dataMat[j,:].T替换为k[i,j]即可，替换效果如下： 复制代码def innerL(i,oS): #计算误差 Ei=calcEk(oS,i) #违背kkt条件 if(((oS.labelMat[i]*Ei&lt;-oS.tol)and(oS.alphas[i]&lt;oS.C))or\ ((oS.labelMat[i]*Ei&gt;oS.tol)and(oS.alphas[i]&gt;0))): j,Ej=selectJ(i,oS,Ei) alphaIold=alphas[i].copy();alphaJold=alphas[j].copy() #计算上下界 if(oS.labelMat[i]!=oS.labelMat[j]): L=max(0,oS.alphas[j]-oS.alphas[i]) H=min(oS.C,oS.C+oS.alphas[j]-oS.alphas[i]) else: L=max(0,oS.alphas[j]+oS.alphas[i]-oS.C) H=min(oS.C,oS.alphas[j]+oS.alphas[i]) if L==H:print(&quot;L==H&quot;);return 0 #计算两个alpha值 eta=2.0*oS.K[i,j]-oS.K[i,i]-oS.K[j,j] if eta&gt;=0:print(&quot;eta&gt;=0&quot;);return 0 oS.alphas[j]-=oS.labelMat[j]*(Ei-Ej)/eta oS.alphas[j]=clipAlpha(oS.alphas[j],H,L) updateEk(oS,j) if(abs(oS.alphas[j]-alphaJold)&lt;0.00001): print(&quot;j not moving enough&quot;);return 0 oS.alphas[i]+=oS.labelMat[j]*oS.labelMat[i]*\ (alphaJold-oS.alphas[j]) updateEk(oS,i) #在这两个alpha值情况下，计算对应的b值 #注，非线性可分情况，将所有内积项替换为核函数K[i,j] b1=oS.b-Ei-oS.labelMat[i]*(oS.alphas[i]-alphaIold)*\ oS.K[i,i]-\ oS.labelMat[j]*(oS.alphas[j]-alphaJold)*\ oS.k[i,j] b2=oS.b-Ej-oS.labelMat[i]*(oS.alphas[i]-alphaIold)*\ oS.k[i,j]-\ oS.labelMat[j]*(oS.alphas[j]-alphaJold)*\ oS.k[i,j] if(0&lt;oS.alphas[i])and (oS.C&gt;oS.alphas[i]):oS.b=b1 elif(0&lt;oS.alphas[j])and (oS.C&gt;oS.alphas[j]):oS.b=b2 else:oS.b=(b1+b2)/2.0 #如果有alpha对更新 return 1 #否则返回0 else return 0 复制代码 有了核函数，我们就能对非线性的数据集进行分类预测了，接下来就是编写代码利用核函数进行测试，需要说明的是，在优化的过程中，我们仅仅需要找到支持向量和其对应的alpha值，而对于其他的样本值可以不用管，甚至可以舍弃，因为这些样本将不会对分类预测函数造成任何影响。这也就是SVM相比KNN算法的优秀的地方所在 复制代码 #测试核函数 #用户指定到达率def testRbf(k1=1.3): #第一个测试集 dataArr,labelArr=loadDataSet(&apos;testSetRBF.txt&apos;) b,alphas=smoP(dataArr,labelArr,200,0.0001,10000,(&apos;rbf&apos;,k1)) dataMat=mat(dataArr);labelMat=mat(labelArr).transpose() svInd=nonzero(alphas.A&gt;0)[0] sVs=dataMat[svInd] labelSV=labelMat[svInd] print(&quot;there are %d Support Vectors&quot;,%shape(sVs)[0]) m,n=shape(dataMat) errorCount=0 for i in range(m): kernelEval=kernelTrans(sVs,dataMat[i,:],(&apos;rbf&apos;,k1)) predict=kernelEval.T*multiply(labelSV,alphas[svInd])+b if sign(predict)!=sign(labelArr[i]):errorCount+=1 print(&quot;the training error rate is: %f&quot;,%(float(errorCount)/m)) #第二个测试集 dataArr,labelArr=loadDataSet(&apos;testSetRBF2.txt&apos;) dataMat=mat(dataArr);labelMat=mat(labelArr).transpose() errorCount=0 m,n=shape(dataMat) for i in range(m): kernelEval=kernelTrans(sVs,dataMat[i,:],(&apos;rbf&apos;,k1)) predict=kernelEval.T*multiply(labelSV,alphas[svInd])+b if sign(predict)!=sign(labelArr[i]):errorCount+=1 print(&quot;the training error rate is: %f&quot;,%(float(errorCount)/m)) 复制代码 当用户输入σ=1.3时的实验结果为： 当σ=0.1时实验结果为： 通过输入不同的σ值（当然，迭代次数也会有一定的影响，我们只讨论σ值），我们发现测试错误率，训练误差率，支持向量个数都会发生变化，在一定的范围内，支持向量数目的下降，会使得训练错误率和测试错误率都下降，但是当抵达某处的最优值时，再次通过增大σ值的方法减少支持向量，此时训练错误率下降，而测试误差上升 简言之，对于固定的数据集，支持向量的数目存在一个最优值，如果支持向量太少，会得到一个很差的决策边界；而支持向量太多，也相当于利用整个数据集进行分类，就类似于KNN算法，显然运算速度不高。 三，SVM实例：手写识别问题 相较于第二张的KNN算法，尽管KNN也能取得不错的效果；但是从节省内存的角度出发，显然SVM算法更胜一筹，因为其不需要保存真个数据集，而只需要其作用的支持向量点，而取得不错的分类效果。 复制代码 #实例：手写识别问题 #支持向量机由于只需要保存支持向量，所以相对于KNN保存整个数据集占用更少内存 #且取得可比的效果 #基于svm的手写数字识别def loadImages(dirName): from os import listdir hwLabels=[] trainingFileList=listdir(dirName) m=len(trainingFileList) trainingMat=zeros((m,1024)) for i in range(m): fileNameStr=trainingFileList[i] fileStr=fileNameStr.split(‘.’)[0] classNumStr=int(fileStr.split(‘_’)[0]) if classNumStr==9:hwLabels.append(-1) else:hwLabels.append(1) trainingMat[i,:]=img2vector(‘%s/%s’,%(dirName,fileNameStr)) return hwLabels,trainingMat #将图像转为向量def img2vector(fileaddir): featVec=zeros((1,1024)) fr=open(filename) for i in range(32): lineStr=fr.readline() for j in range(32): featVec[0,32*i+j]=int(lineStr[j]) return featVec #利用svm测试数字def testDigits(kTup=(‘rbf’,10)): #训练集 dataArr,labelArr=loadDataSet(&apos;trainingDigits&apos;) b,alphas=smoP(dataArr,labelArr,200,0.0001,10000,kTup) dataMat=mat(dataArr);labelMat=mat(labelArr).transpose() svInd=nonzero(alphas.A&gt;0)[0] sVs=dataMat[svInd] labelSV=labelMat[svInd] print(&quot;there are %d Support Vectors&quot;,%shape(sVs)[0]) m,n=shape(dataMat) errorCount=0 for i in range(m): kernelEval=kernelTrans(sVs,dataMat[i,:],kTup) predict=kernelEval.T*multiply(labelSV,alphas[svInd])+b if sign(predict)!=sign(labelArr[i]):errorCount+=1 print(&quot;the training error rate is: %f&quot;,%(float(errorCount)/m)) #测试集 dataArr,labelArr=loadDataSet(&apos;testDigits.txt&apos;) dataMat=mat(dataArr);labelMat=mat(labelArr).transpose() errorCount=0 m,n=shape(dataMat) for i in range(m): kernelEval=kernelTrans(sVs,dataMat[i,:],(&apos;rbf&apos;,k1)) predict=kernelEval.T*multiply(labelSV,alphas[svInd])+b if sign(predict)!=sign(labelArr[i]):errorCount+=1 print(&quot;the training error rate is: %f&quot;,%(float(errorCount)/m)) 复制代码下面来看一下，在kTup=(‘rbf’,20)情况下的测试误差率和支持向量个数情况 并且通过尝试不同的σ值，以及尝试了线性核函数，可以得到关于不同σ值的书写数字识别性能： 内核模式，设置 训练错误率(%) 测试错误率(%) 支持向量数rbf,0.1 0 52 402rbf,5 0 3.2 402rbf,10 0 0.5 99rbf,50 0.2 2.2 41rbf,100 4.5 4.3 26Linear 2.7 2.2 38 由上图可以看出，σ值在取10时取得了最好的分类效果，这也印证了我们上面的叙述。即对于固定的数据集，存在最优的支持向量个数，使得分类错误率最低。支持向量的个数会随着σ值的增大而逐渐减少，但是分类错误率确实一个先降低后升高的过程。即最小的分类错误率并不意味着最少的支持向量个数。 4 总结 支持向量机是一种通过求解凸二次规划问题来解决分类问题的算法，具有较低的泛化错误率。而SMO算法可以通过每次只优化两个alpha值来加快SVM的训练速度。 核技巧是将数据由低维空间映射到高维空间，可以将一个低维空间中的非线性问题转换为高维空间下的线性问题来求解。而径向基核函数是一个常用的度量两个向量距离的核函数。 最后，支持向量机的优缺点： 优点：泛化错误率低，计算开销不大 缺点：对参数调节和核函数的选择敏感，且仅适用于二类分类]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[港版书《你快乐所以你成功》]]></title>
    <url>%2F2018%2F01%2F16%2F%E6%B8%AF%E7%89%88%E4%B9%A6%E3%80%8A%E4%BD%A0%E5%BF%AB%E4%B9%90%E6%89%80%E4%BB%A5%E4%BD%A0%E6%88%90%E5%8A%9F%E3%80%8B%2F</url>
    <content type="text"><![CDATA[推荐语：成功不一定快乐，但快乐更容易成功前段时间途径香港尖沙咀诚品书店，畅销书一栏处一墙畅销书，默默然拿起来这本。一看书名是个很成功学的书籍，但是他的封面的简约与灵动反而更显内涵。香港社会学研究很深入，而且心理学研究水平很好，姑且我相信这是一本好书。内容是斯坦福大学的快乐心理学课程，主题为打破6大惯性成功迷思，化快乐为生产力。大陆地区经济颇为繁华，但是有个恒久的问题就是：谁说吃苦才能让你更成功？善用正念，韧性，能量管理，偷闲放空，宽容和仁慈的力量。 耐力，毅力都有非常辛苦的意思，耐力是做你不喜欢，不快乐的事，但非做不可，过程非常辛苦，需要强大的耐力支撑；毅力是去做你喜欢，快乐的事，过程也非常辛苦，但是你却不以为意。正是一种这样的精神，‘成功者不一定快乐，但快乐的人却更容易成功’。我深深同意这句话。当年我喜欢看《三国演义》，三四厘米厚的书，旁人会觉得，天呀，怎么看得完，但是我越看越喜欢，看了四五遍也意犹未尽。这就是快乐与成功的现实关系。当人追逐金钱，权利，地位，买车买房，娶妻生子后的人生，过度的金钱与豪奢，都会让人进入另一种恶性循环，进而]]></content>
  </entry>
  <entry>
    <title><![CDATA[2018年]]></title>
    <url>%2F2018%2F01%2F16%2F2018%E5%B9%B4%2F</url>
    <content type="text"><![CDATA[你好吗。很久没来这里了。最近很忙呀。学车，机器学习，算法刷题，iOS实战案例操作，数据结构，毕业设计，课外书。又是新的一年。爸妈老了一岁。自己又大了一岁。世界还是在变化。跟小女友的关系也越来越好。了解得越来越深。越来越有默契。也越来越来愿景。越来越温馨。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F09%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
